{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code and Documentation to Decode Popup Buoy Transmitted/Recorded Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Requirements\n",
    "\n",
    "Developed/tested for:\n",
    "- python >=3.6 \n",
    "\n",
    "requires:\n",
    "- pyyaml >= 3.13 \n",
    "- pandas >= 0.23.4\n",
    "- numpy >= 1.15.4\n",
    "- matplotlib >= 3.0.2\n",
    "- jupyterlab >= 1.0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import collections\n",
    "import os\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import YearLocator, WeekdayLocator, MonthLocator, DayLocator, HourLocator, DateFormatter\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import NullFormatter  # useful for `logit` scale\n",
    "\n",
    "import datetime\n",
    "from netCDF4 import num2date, date2num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure\n",
    "\n",
    "Data downloaded from Popup Buoy's directly, generate 8 binary files.  A complete description of these files can be found at **[link]**.  Relevant pieces will be included in the notebook.\n",
    "\n",
    "Filenames:\n",
    "- BOTDAT.TXT\n",
    "- FILEPOS.TXT\n",
    "- ICEDAT.TXT\n",
    "- JPGxxxxx.JPG\n",
    "- PRODAT.TXT\n",
    "- SSTDAT.TXT\n",
    "- SUMMARY.TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['300434063470010', '300434063474010', '300434063477010', '300434063479200', '300434063861360', '300434063924230']\n"
     ]
    }
   ],
   "source": [
    "#list id's in download path\n",
    "root_path = os.path.join(\"..\", \"rawdata\", \"ecofoci.popupsbd\")\n",
    "\n",
    "id_dir = os.listdir(root_path)\n",
    "print(id_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# year deployed - year of data tx/rx - imei number - site: SBD Active/Inactive \n",
    "\n",
    "# 2018 - 2019 - 300434063921240 - C2: Inactive\n",
    "# 2018 - 2019 - 300434063823800 - C10/C11: Inactive\n",
    "# 2018 - 2019 - 300434063928220 - C12: Inactive\n",
    "# 2018 - 2019 - 300434063925210 - M5: Inactive\n",
    "\n",
    "# 2019 - 2020 - 300434063470010 - S.W. of M5: Inative\n",
    "# 2019 - 2020 - 300434063477010 - N.W. of M5: Inactive\n",
    "# 2019 - 2020 - 300434063861360 - N.E. of Saint Lawrence: Inactive\n",
    "# 2019 - 2021 - 300434063474010 - M8 PopTop: Inactive\n",
    "\n",
    "#select unit and year of interest\n",
    "#instid = '300434063470010' \n",
    "\n",
    "\n",
    "#INPUT the imei number of the unit(s) of interest. Can list multiple, separated by a comma (as long as year of data transmission and deployment are the same)\n",
    "instid = '300434063474010'\n",
    "\n",
    "#INPUT the year of data transmission you are interested in looking at\n",
    "tx = '2021'\n",
    "#INPUT the year the float was deployed\n",
    "year = '2019'\n",
    "\n",
    "print(type(year))\n",
    "print(type(instid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\yaml\\300434063474010.yaml\n"
     ]
    }
   ],
   "source": [
    "results = os.path.join('..', 'results', year, instid)\n",
    "\n",
    "instfile_dic = {'bot_file': os.path.join(results, 'BOTDAT.TXT'),\n",
    "                'ice_file': os.path.join(results, 'ICEDAT.TXT'),\n",
    "                'pro_file': os.path.join(results, 'PRODAT.TXT'),\n",
    "                'sst_file': os.path.join(results, 'SSTDAT.TXT')}\n",
    "\n",
    "instconfig = os.path.join('..','yaml', instid + '.yaml')\n",
    "\n",
    "print(instconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each data file, we are going to read in the entire file and convert the binary to hex.  There are multiple line, lengths we are going to have to address but the start of each record is denoted by 'FFFF'.  We can split the filestring on this parameter but we need to be aware of 'FFFFF' or 'FFFFFF' posibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Basic Approach***\n",
    "\n",
    "The two modules below will allow for a simple readin of the file for very simple analysis and debugging... the code of consequence that involves conversion of measurements from engineering units to science units as all defined in the PopUpBuoy CLASS in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HexView(file):\n",
    "    with open(file, 'rb') as in_file:\n",
    "        while True:\n",
    "            hexdata = in_file.read().hex().upper()     # Read the shortest possible line\n",
    "            if len(hexdata) == 0:                      # breaks loop once no more binary data is read\n",
    "                break\n",
    "            \n",
    "            return(hexdata.upper())\n",
    "        \n",
    "def HexSplit(hexstr):\n",
    "    if hexstr.find('FFFFF') == -1:\n",
    "        print(\"No FFFFF, proceed to split on FFFF\")\n",
    "        sample_raw = hexstr.split('FFFF')[1:]\n",
    "    else:\n",
    "        print('FFFFF found')\n",
    "        #this puts in the proper line endings but removes a variable \n",
    "        #   F from the end of each string.  Add the F string back\n",
    "        sample_raw = []\n",
    "        for substr in hexstr.split('FFFFF')[1:]: \n",
    "            sample_raw = sample_raw + (substr + 'F').split('FFFF')\n",
    "\n",
    "        sample_raw[-1] = sample_raw[-1][:-1]\n",
    "        \n",
    "    return(sample_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No FFFFF, proceed to split on FFFF\n",
      "..\\results\\2019\\300434063474010\\BOTDAT.TXT\n"
     ]
    }
   ],
   "source": [
    "active_file = instfile_dic['bot_file']\n",
    "\n",
    "hexstr = HexView(active_file)\n",
    "sample_raw = HexSplit(hexstr)\n",
    "\n",
    "print(active_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class description and routine code\n",
    "\n",
    "## Decode sample data for each file type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottom Data (BOTDAT.TXT) / Under Ice Data (ICEDAT.TXT)\n",
    "\n",
    "This data has two record lengths.  17 and 19.  Since we've split on 'FFFF' and broken the record up into samples, the initial 2bytes are no longer in the record so the record lengths are now 15 and 17 (which is a string length of 30 and 34 characters)\n",
    "\n",
    "***MSG Decode Key***\n",
    "![BotDecodeMsg](decode_images/BotDat_msg_decode.png)\n",
    "\n",
    "***Engineering to Science Conversions***\n",
    "![BotDecodeMsg](decode_images/BotDat_msg_cal.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile Data (PRODAT.TXT) /\n",
    "\n",
    "This data has two record lengths.  13 and 15.  Since we've split on 'FFFF' and broken the record up into samples, the initial 2bytes are no longer in the record so the record lengths are now 11 and 13 (which is a string length of 26 and 30 characters).  This file does not have the bottom temp or the reference temp fields.\n",
    "\n",
    "***MSG Decode Key***\n",
    "![ProDecodeMsg](decode_images/ProDat_msg_decode.png)\n",
    "\n",
    "***Engineering to Science Conversions***\n",
    "![ProCalMsg](decode_images/ProDat_msg_cal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SST Data (SSTDAT.TXT) /\n",
    "\n",
    "This data has eight record lengths.  17 and 35, 18 and 36, 19 and 37 or 20 and 38.  Since we've split on 'FFFF' and broken the record up into samples, the initial 2bytes are no longer in the record so the record lengths are now 15 and 33, 16 and 34, 17 and 35 or 18 and 36.  The 8 types of files are if there is the short time format or the long time format, if TTS is provided or not, and if there is gps encoding or not for each of the time formats.\n",
    "\n",
    "17: short-time, no gps, no TTS   \n",
    "18: short-time, no gps, TTS   \n",
    "19: long-time, no gps, no TTS   \n",
    "20: long-time, no gps, TTS   \n",
    "35: short-time, gps, no TTS   \n",
    "36: short-time, gps, TTS   \n",
    "37: long-time, gps, no TTS   \n",
    "38: long-time, gps, TTS   \n",
    "\n",
    "**UPDATE 3-12-2019:**  TTF is also an option now as an additional record of 2 bytes\n",
    "\n",
    "***MSG Decode Key***\n",
    "![SSTDecodeMsg](decode_images/SSTDat_msg_decode.png)\n",
    "\n",
    "***Engineering to Science Conversions***\n",
    "![SSTDecodeMsg](decode_images/SSTDat_msg_cal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the PopUpBuoy Class\n",
    "\n",
    "import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preload matplotlib plot paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### specify primary bulk figure parameters\n",
    "fontsize = 10\n",
    "labelsize = 10\n",
    "plotstyle = 'seaborn'\n",
    "max_xticks = 10\n",
    "plt.style.use('seaborn-ticks')\n",
    "mpl.rcParams['svg.fonttype'] = 'none'\n",
    "mpl.rcParams['ps.fonttype'] = 42 #truetype/type2 fonts instead of type3\n",
    "mpl.rcParams['pdf.fonttype'] = 42 #truetype/type2 fonts instead of type3\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "mpl.rcParams['axes.edgecolor'] = 'black'\n",
    "mpl.rcParams['axes.linewidth'] = 1.5\n",
    "mpl.rcParams['axes.labelcolor'] = 'black'\n",
    "mpl.rcParams['grid.linestyle'] = '--'\n",
    "mpl.rcParams['grid.linestyle'] = '--'\n",
    "mpl.rcParams['xtick.major.size'] = 4\n",
    "mpl.rcParams['xtick.minor.size'] = 2\n",
    "mpl.rcParams['xtick.major.width'] = 2\n",
    "mpl.rcParams['xtick.minor.width'] = 0.5\n",
    "mpl.rcParams['ytick.major.size'] = 4\n",
    "mpl.rcParams['ytick.minor.size'] = 2\n",
    "mpl.rcParams['ytick.major.width'] = 2\n",
    "mpl.rcParams['ytick.minor.width'] = 0.5\n",
    "mpl.rcParams['ytick.direction'] = 'out'\n",
    "mpl.rcParams['xtick.direction'] = 'out'\n",
    "mpl.rcParams['ytick.color'] = 'black'\n",
    "mpl.rcParams['xtick.color'] = 'black'\n",
    "\n",
    "mpl.rcParams['contour.negative_linestyle'] = 'solid'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signed_int(hexstr):\n",
    "    '''function to convert hex string to signed int'''\n",
    "    s_int = int(hexstr,16)\n",
    "    if s_int >= 0x8000:\n",
    "        s_int -= 0x10000  \n",
    "\n",
    "    return(s_int)\n",
    "\n",
    "class PopUpBuoys(object):\n",
    "    \"\"\"Class definitions to read and Process PopUp Buoy Data Streams\"\"\"\n",
    "\n",
    "\n",
    "    active_stream = 'bottom'\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.instfile_dic = {'bottom': path + '/BOTDAT.TXT',\n",
    "                             'ice': path + '/ICEDAT.TXT',\n",
    "                             'profile': path + '/PRODAT.TXT',\n",
    "                             'sst': path + '/SSTDAT.TXT'}\n",
    "    \n",
    "    def LoadCoefs(self, config_file = 'default.yaml'):\n",
    "        ''' Load yaml formated config file '''\n",
    "        self.config = yaml.safe_load(open(config_file))\n",
    "\n",
    "    \n",
    "    def HexView(self, active_stream='', verbose=True):\n",
    "        '''\n",
    "        input: reference to proper filepointer, options are keys \n",
    "            to the self.instfile_dic dictionary\n",
    "        '''\n",
    "        if active_stream:\n",
    "            self.active_stream = active_stream\n",
    "            \n",
    "        file = self.instfile_dic[self.active_stream]\n",
    "        with open(file, 'rb') as in_file:\n",
    "            while True:\n",
    "                hexdata = in_file.read().hex().upper()     \n",
    "                if len(hexdata) == 0:                      \n",
    "                # breaks loop once no more binary data is read\n",
    "                    break\n",
    "                self.hexstr = hexdata.upper()\n",
    "                \n",
    "                if verbose:\n",
    "                    return(hexdata.upper())\n",
    "\n",
    "    def HexSplit(self, verbose=True):\n",
    "        '''\n",
    "        input: results of HexView (inherits output)\n",
    "        '''\n",
    "        if self.hexstr.find('FFFFF') == -1:\n",
    "            print(\"No FFFFF, proceed to split on FFFF\")\n",
    "            #because in this case the first index is empty\n",
    "            sample_raw = self.hexstr.split('FFFF')[1:] \n",
    "        else:\n",
    "            print('FFFFF found')\n",
    "            #this puts in the proper line endings but removes a variable \n",
    "            #   F from the end of each string.  Add the F string back\n",
    "            sample_raw = []\n",
    "            for substr in self.hexstr.split('FFFFF'): \n",
    "                sample_raw = sample_raw + (substr + 'F').split('FFFF')\n",
    "\n",
    "            sample_raw[-1] = sample_raw[-1][:-1]\n",
    "        \n",
    "        self.sample_raw = sample_raw\n",
    "        \n",
    "        if verbose:\n",
    "            return(sample_raw)\n",
    "    \n",
    "    def Bottom(self, asPandas=False):\n",
    "        ''' Bottom is equivalent to the TimeSeriesBase'''\n",
    "        \n",
    "        if self.active_stream != 'bottom':\n",
    "            print(\"current active file is {} - can't output bottom data\".format(self.active_stream))\n",
    "            return\n",
    "\n",
    "        data = self.TimeSeriesBase()\n",
    "\n",
    "        if asPandas:\n",
    "            data = pd.DataFrame.from_dict(data,orient='index')\n",
    "        return(data)\n",
    "    \n",
    "\n",
    "    def Profile(self, asPandas=False):\n",
    "        try:\n",
    "            self.sample_raw\n",
    "        except:\n",
    "            print(\"Run PopUpBuoys.HexView and PopUpBuoys.HexSplit First\")\n",
    "            return\n",
    "\n",
    "        if self.active_stream != 'profile':\n",
    "            print(\"current active file is {} - can't output profile data\".format(self.active_stream))\n",
    "            return\n",
    "                        \n",
    "        data=collections.OrderedDict()\n",
    "\n",
    "        for sample_num, sample in enumerate(self.sample_raw):\n",
    "                        \n",
    "            if len(sample) == 22: #2byte timeword\n",
    "                \n",
    "                # unlike sst, bottom, and ice - profiles don't need to be multiplied by a sample interval\n",
    "                #seconds since 1970-01-01\n",
    "                try:\n",
    "                    time = int(sample[0:4],16)/100 + profile_starttime\n",
    "                except:\n",
    "                    time = int(sample[0:4],16)/100 + 0\n",
    "                    \n",
    "                pressure =  self.PressureConversion(int(sample[4:8],16)) \n",
    "                \n",
    "                ttp_temp_ADC = signed_int(sample[8:12])\n",
    "                ttp_temp = self.TempConversion(engr_meas=ttp_temp_ADC,\n",
    "                                                  coefA=self.config['ttp_temp_cal']['Acoef'], \n",
    "                                                  coefB=self.config['ttp_temp_cal']['Bcoef'],\n",
    "                                                  coefC=self.config['ttp_temp_cal']['Ccoef'])\n",
    "\n",
    "                rawpvalue = signed_int(sample[12:16])                  \n",
    "                par = self.PARConversion(engr_meas=rawpvalue,\n",
    "                                        coef_offset=self.config['par_cal']['offset'], \n",
    "                                        coef_slope=self.config['par_cal']['slope'])   \n",
    "\n",
    "                rawfvalue = signed_int(sample[16:20])                   \n",
    "                fluor = self.FluorConversion(engr_meas=rawfvalue,\n",
    "                                        coef_offset=self.config['fluor_cal']['offset'], \n",
    "                                        coef_slope=self.config['fluor_cal']['slope'])  \n",
    "\n",
    "                tilt = int(sample[20:22],16) #degrees\n",
    "                \n",
    "            elif len(sample) == 26: #4byte timeword, reserved for profile starttime\n",
    "                \n",
    "                #seconds since 1970-01-01\n",
    "                profile_starttime = int(sample[0:8],16)\n",
    "                \n",
    "                time = profile_starttime\n",
    "                \n",
    "                pressure =  self.PressureConversion(int(sample[8:12],16))  \n",
    "                \n",
    "                ttp_temp_ADC = signed_int(sample[12:16])\n",
    "                ttp_temp = self.TempConversion(engr_meas=ttp_temp_ADC,\n",
    "                                                  coefA=self.config['ttp_temp_cal']['Acoef'], \n",
    "                                                  coefB=self.config['ttp_temp_cal']['Bcoef'],\n",
    "                                                  coefC=self.config['ttp_temp_cal']['Ccoef'])\n",
    "                \n",
    "                rawpvalue = signed_int(sample[16:20])             \n",
    "                par = self.PARConversion(engr_meas=rawpvalue,\n",
    "                                        coef_offset=self.config['par_cal']['offset'], \n",
    "                                        coef_slope=self.config['par_cal']['slope'])  \n",
    "\n",
    "                rawfvalue = signed_int(sample[20:24])                \n",
    "                fluor = self.FluorConversion(engr_meas=rawfvalue,\n",
    "                                        coef_offset=self.config['fluor_cal']['offset'], \n",
    "                                        coef_slope=self.config['fluor_cal']['slope'])  \n",
    "\n",
    "                tilt = int(sample[24:26],16) #degrees\n",
    "            \n",
    "            elif len(sample) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                continue # skip lines with too many data points\n",
    "            #save to dictionary\n",
    "            data[sample_num] = {'time':time,\n",
    "                                'pressure':pressure,\n",
    "                                'ttp_temp':ttp_temp,\n",
    "                                'ttp_temp_ADC':ttp_temp_ADC,\n",
    "                                'par':par,\n",
    "                                'fluor':fluor,\n",
    "                                'tilt':tilt}        \n",
    "        \n",
    "        if asPandas:\n",
    "            data = pd.DataFrame.from_dict(data,orient='index')\n",
    "        return(data)\n",
    "        \n",
    "    def Ice(self, asPandas=False):\n",
    "        ''' Ice is equivalent to the TimeSeriesBase'''\n",
    "\n",
    "        if self.active_stream != 'ice':\n",
    "            print(\"current active file is {} - can't output ice data\".format(self.active_stream))\n",
    "            return\n",
    "\n",
    "        data = self.TimeSeriesBase()\n",
    "\n",
    "        if asPandas:\n",
    "            data = pd.DataFrame.from_dict(data,orient='index')\n",
    "        return(data)\n",
    "    \n",
    "    def SST(self, asPandas=False):\n",
    "        ''' SST is equivalent to the TimeSeriesBase + GPS information'''\n",
    "        \n",
    "        if self.active_stream != 'sst':\n",
    "            print(\"current active file is {} - can't output sst data\".format(self.active_stream))\n",
    "            return\n",
    "\n",
    "        data = self.TimeSeriesBase()\n",
    "\n",
    "\n",
    "        for sample_num, sample in enumerate(self.sample_raw):\n",
    "\n",
    "            if (len(sample) == 30) or (len(sample) == 66): #2byte timeword, no TTS (w and w/o gps)\n",
    "                TTS = np.nan\n",
    "                if len(sample) == 66:\n",
    "                    print(\"analyze GPS\")\n",
    "                    gps_data = self.GPSSeriesBase(sample_num, sample, empty=False)\n",
    "                else:\n",
    "                    gps_data = self.GPSSeriesBase(sample_num, sample, empty=True)\n",
    "\n",
    "            elif (len(sample) == 32) or (len(sample) == 68): #2byte timeword, yes TTS (w and w/o gps)\n",
    "                TTS = int(sample[30:32],16)\n",
    "                if len(sample) == 68:\n",
    "                    gps_data = self.GPSSeriesBase(sample_num, sample, offset=2, empty=False)\n",
    "                else:\n",
    "                    gps_data = self.GPSSeriesBase(sample_num, sample, empty=True)\n",
    "\n",
    "            elif (len(sample) == 34) or (len(sample) == 70): #4byte timeword, no TTS (w and w/o gps)\n",
    "                TTS = np.nan\n",
    "                if len(sample) == 70:\n",
    "                    gps_data = self.GPSSeriesBase(sample_num, sample, offset=4, empty=False)\n",
    "                else:\n",
    "                    gps_data = self.GPSSeriesBase(sample_num, sample, empty=True)\n",
    "\n",
    "            elif (len(sample) == 36) or (len(sample) == 72): #4byte timeword, yes TTS (w and w/o gps)\n",
    "                TTS = int(sample[30:32],16)\n",
    "                if len(sample) == 72:\n",
    "                    gps_data = self.GPSSeriesBase(sample_num, sample, offset=6, empty=False)\n",
    "                else:\n",
    "                    gps_data = self.GPSSeriesBase(sample_num, sample, empty=True)\n",
    "\n",
    "            elif len(sample) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                continue # skip lines with too many data points\n",
    "            #save to dictionary\n",
    "\n",
    "            try:\n",
    "                data[sample_num].update({'TTS':TTS})        \n",
    "                data[sample_num].update(gps_data[sample_num])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if asPandas:\n",
    "            return(pd.DataFrame.from_dict(data,orient='index'))\n",
    "        else:\n",
    "            return(data)\n",
    "\n",
    "\n",
    "    ### The folowing BASE functions are for convenience for reading and coding.  All redundant\n",
    "    # pattern reads are below.  The only challenge is that each BASE function reads the entire hex\n",
    "    # string (non-issue for files of size we expect)\n",
    "    def GPSSeriesBase(self,sample_num, sample, offset=0, empty=True):\n",
    "        '''SST only, the last 18 bytes are the same format:\n",
    "         GPSLat, GPSLon, GPSDate, GPSTime, TTF, Max_Tilt\n",
    "         Regardless of whether TTS or longdates are used.  This subroutine returns the \n",
    "         GPS dictionary based on a byte offset given the record length\n",
    "\n",
    "         Passing empty=True sends missing data back for GPS Data'''\n",
    "        data=collections.OrderedDict()\n",
    "\n",
    "        if not empty:\n",
    "            GPSLat = int(sample[30+offset:38+offset],16)/1000000\n",
    "            GPSLon = (int(sample[38+offset:46+offset],16)- 4294967295) / 1000000\n",
    "            GPSDate = int(sample[46+offset:54+offset],16)\n",
    "            GPSTime = int(sample[54+offset:62+offset],16)\n",
    "            TTF = int(sample[62+offset:64+offset],16)\n",
    "            Max_Tilt = int(sample[62+offset:66+offset],16)   \n",
    "\n",
    "        else:\n",
    "            GPSLat = GPSLon = GPSDate = GPSTime = TTF = Max_Tilt = np.nan \n",
    "            \n",
    "        #save to dictionary\n",
    "        data[sample_num] = {'GPSLat':GPSLat,\n",
    "                            'GPSLon':GPSLon,\n",
    "                            'GPSDate':GPSDate,\n",
    "                            'GPSTime':GPSTime,\n",
    "                            'TTF':TTF,\n",
    "                            'Max_Tilt':Max_Tilt}  \n",
    "            \n",
    "        return(data)   \n",
    "\n",
    "    def TimeSeriesBase(self):\n",
    "        '''Bottom, Ice, and SST all have the same base transmission information,\n",
    "            e.g. the first 17 bytes (short time stamp) / 19 bytes (long time stamp)\n",
    "            are the same.  Each of the the appropriate modules will call this communal module\n",
    "            first.\n",
    "\n",
    "            Bottom and Ice don't report any additional information beyond the base info so \n",
    "            they are essential decorators/wrappers for this function\n",
    "\n",
    "            Returns: Dictionary'''\n",
    "        try:\n",
    "            self.sample_raw\n",
    "        except:\n",
    "            print(\"Run PopUpBuoys.HexView and PopUpBuoys.HexSplit First\")\n",
    "            return\n",
    "\n",
    "        data=collections.OrderedDict()\n",
    "\n",
    "        for sample_num, sample in enumerate(self.sample_raw):\n",
    "            \n",
    "            #record length conditionals are due to number of varying outputs\n",
    "            #SST dominates the number of options due to:\n",
    "            # 2byte timeword, no TTS, no GPS (30)\n",
    "            # 4byte timeword, yes TTS, yes GPS (72)\n",
    "            #  and every permutation of the three functions\n",
    "            if ((len(sample) == 30) or (len(sample) == 32) or \n",
    "                            (len(sample) == 60) or (len(sample) == 64) or (len(sample) == 66) or (len(sample) == 68)) : #2byte timeword\n",
    "                \n",
    "                #seconds since 1970-01-01\n",
    "                if self.active_stream in ['sst']:\n",
    "                    #the correction is available for any non-utc setup pc\n",
    "                    time_base = date2num(datetime.datetime.strptime(IDNUMBER.config['Unit_Release_Time'],\n",
    "                                        '%Y-%m-%d %H:%M:%S'),\n",
    "                                                                    'seconds since 1970-1-1')\n",
    "                else:\n",
    "                    time_base = 0 \n",
    "\n",
    "                time = int(sample[0:4],16) * self.config['sample_interval'][self.active_stream] + time_base\n",
    "                \n",
    "                pressure =  self.PressureConversion(int(sample[4:8],16))  \n",
    "                \n",
    "                ttp_temp_ADC = signed_int(sample[8:12])\n",
    "                ttp_temp = self.TempConversion(engr_meas=ttp_temp_ADC,\n",
    "                                                  coefA=self.config['ttp_temp_cal']['Acoef'], \n",
    "                                                  coefB=self.config['ttp_temp_cal']['Bcoef'],\n",
    "                                                  coefC=self.config['ttp_temp_cal']['Ccoef'])\n",
    "\n",
    "                sst_temp_ADC = signed_int(sample[12:16])\n",
    "                sst_temp = self.TempConversion(engr_meas=sst_temp_ADC,\n",
    "                                                  coefA=self.config['sst_temp_cal']['Acoef'], \n",
    "                                                  coefB=self.config['sst_temp_cal']['Bcoef'],\n",
    "                                                  coefC=self.config['sst_temp_cal']['Ccoef'])\n",
    "\n",
    "                temp_ref = signed_int(sample[16:20])               \n",
    "                \n",
    "                rawpvalue = signed_int(sample[20:24])               \n",
    "                par = self.PARConversion(engr_meas=rawpvalue,\n",
    "                                        coef_offset=self.config['par_cal']['offset'], \n",
    "                                        coef_slope=self.config['par_cal']['slope']) \n",
    "                \n",
    "                rawfvalue = signed_int(sample[24:28])                 \n",
    "                fluor = self.FluorConversion(engr_meas=rawfvalue,\n",
    "                                        coef_offset=self.config['fluor_cal']['offset'], \n",
    "                                        coef_slope=self.config['fluor_cal']['slope'])  \n",
    "\n",
    "                tilt = int(sample[28:30],16) #degrees\n",
    "                \n",
    "            elif ((len(sample) == 34) or (len(sample) == 36) or (len(sample) == 72)) : #4byte timeword\n",
    "                \n",
    "                #seconds since 1970-01-01\n",
    "                time = int(sample[0:8],16)\n",
    "                \n",
    "                pressure =  self.PressureConversion(int(sample[8:12],16))   \n",
    "                \n",
    "                ttp_temp_ADC = signed_int(sample[12:16])\n",
    "                ttp_temp = self.TempConversion(engr_meas=ttp_temp_ADC,\n",
    "                                                  coefA=self.config['ttp_temp_cal']['Acoef'], \n",
    "                                                  coefB=self.config['ttp_temp_cal']['Bcoef'],\n",
    "                                                  coefC=self.config['ttp_temp_cal']['Ccoef'])\n",
    "\n",
    "                sst_temp_ADC = signed_int(sample[16:20])\n",
    "                sst_temp = self.TempConversion(engr_meas=sst_temp_ADC,\n",
    "                                                  coefA=self.config['sst_temp_cal']['Acoef'], \n",
    "                                                  coefB=self.config['sst_temp_cal']['Bcoef'],\n",
    "                                                  coefC=self.config['sst_temp_cal']['Ccoef'])\n",
    "\n",
    "                temp_ref = signed_int(sample[20:24])          \n",
    "                \n",
    "                rawpvalue = signed_int(sample[24:28])                 \n",
    "                par = self.PARConversion(engr_meas=rawpvalue,\n",
    "                                        coef_offset=self.config['par_cal']['offset'], \n",
    "                                        coef_slope=self.config['par_cal']['slope'])  \n",
    "\n",
    "                rawfvalue = signed_int(sample[28:32])               \n",
    "                fluor = self.FluorConversion(engr_meas=rawfvalue,\n",
    "                                        coef_offset=self.config['fluor_cal']['offset'], \n",
    "                                        coef_slope=self.config['fluor_cal']['slope'])  \n",
    "                \n",
    "                tilt = int(sample[32:34],16) #degrees            \n",
    "\n",
    "            elif len(sample) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                continue # skip lines with too many data points\n",
    "            #save to dictionary\n",
    "            data[sample_num] = {'time':time,\n",
    "                                'pressure':pressure,\n",
    "                                'ttp_temp':ttp_temp,\n",
    "                                'ttp_temp_ADC':ttp_temp_ADC,\n",
    "                                'sst_temp':sst_temp,\n",
    "                                'sst_temp_ADC':sst_temp_ADC,\n",
    "                                'temp_ref':temp_ref,\n",
    "                                'par':par,\n",
    "                                'fluor':fluor,\n",
    "                                'tilt':tilt}  \n",
    "        return(data)\n",
    "\n",
    "    @staticmethod\n",
    "    def PARConversion(engr_meas,coef_offset, coef_slope):\n",
    "        ''' Calculate PAR from raw measurement\n",
    "        \n",
    "            (ADC_val - coef_offset) * coef_slope / 0.73\n",
    "\n",
    "            output is PAR in umolm-2s-1\n",
    "        '''\n",
    "        return((engr_meas - coef_offset) * coef_slope / 0.73 )\n",
    "\n",
    "    @staticmethod\n",
    "    def FluorConversion(engr_meas,coef_offset, coef_slope):\n",
    "        ''' Calculate Fluometer from raw measurement\n",
    "\n",
    "            (ADC_val - coef_offset) * coef_slope\n",
    "        \n",
    "            output is concentration in ug/L\n",
    "        '''\n",
    "        return((engr_meas - coef_offset) * coef_slope) \n",
    "\n",
    "    @staticmethod\n",
    "    def PressureConversion(engr_meas):\n",
    "        ''' Calculate Pressure from raw measurement\n",
    "\n",
    "            (ADC_val - 16384) * 10 / 32768\n",
    "        \n",
    "            output is Pressure in Bars\n",
    "        '''\n",
    "        return((engr_meas - 16384) * 10 / 32768)\n",
    "\n",
    "    @staticmethod\n",
    "    def TempConversion(engr_meas, coefA, coefB, coefC, version=1):\n",
    "        ''' Calculate Temperature from raw measurement.\n",
    "\n",
    "        1 / ( coefA + \n",
    "              coefB*np.log(ADC_val) + \n",
    "              coefC*np.log(ADC_val)^3 ) - 273.15 \n",
    "\n",
    "        Output is Temperature in DegC\n",
    "        '''\n",
    "        if version == 1:\n",
    "            temperature = 1 / ( coefA + \n",
    "                         coefB*np.log10(engr_meas) + \n",
    "                         coefC*np.log10(engr_meas)**3 ) - 273.15 \n",
    "        if version == 2:\n",
    "            temperature = 1 / ( coefA + \n",
    "                         coefB*np.log(engr_meas) + \n",
    "                         coefC*np.log(engr_meas)**3 ) - 273.15       \n",
    "\n",
    "        return(temperature)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Evaluation of routine\n",
    "\n",
    "Imagine a buoy with ID number **xxxxx**.  Instantiate a PopUpBuoys class with the relative (or absolute) path to the location of the download/reconstructed data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\results\\2019\\300434063474010\n",
      "<__main__.PopUpBuoys object at 0x000001C839C324E0>\n"
     ]
    }
   ],
   "source": [
    "IDNUMBER = PopUpBuoys(results)\n",
    "print(results)\n",
    "print(IDNUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the configuration file.  If not specified it will load a file named 'default.yaml' in the same path as the utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'safe_load'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-d579045b5515>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mIDNUMBER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoadCoefs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minstconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-68-9883d742c891>\u001b[0m in \u001b[0;36mLoadCoefs\u001b[1;34m(self, config_file)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mLoadCoefs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;34m''' Load yaml formated config file '''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'safe_load'"
     ]
    }
   ],
   "source": [
    "IDNUMBER.LoadCoefs(config_file=instconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the routine to read and convert the binary file to a hex string... the sample parameter is the name of the data type.\n",
    "\n",
    "active_stream options are:\n",
    "+ bottom\n",
    "+ sst\n",
    "+ profile\n",
    "+ ice\n",
    "\n",
    "passing 'verbose=True' returns the hex string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDNUMBER.HexView(active_stream='bottom',verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDNUMBER.HexSplit(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_data = IDNUMBER.Bottom(asPandas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_data.ttp_temp.plot()\n",
    "#bottom_data.pressure.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDNUMBER.HexView(active_stream='ice',verbose=False)\n",
    "IDNUMBER.HexSplit(verbose=False)\n",
    "ice_data = IDNUMBER.Ice(asPandas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDNUMBER.HexSplit(verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDNUMBER.Ice(asPandas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hxv = IDNUMBER.HexView(active_stream='profile',verbose=True)\n",
    "hs = IDNUMBER.HexSplit(verbose=True)\n",
    "pro_data = IDNUMBER.Profile(asPandas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs[98:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pro_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hxv = IDNUMBER.HexView(active_stream='sst',verbose=True)\n",
    "hs = IDNUMBER.HexSplit(verbose=True)\n",
    "sst_data = IDNUMBER.SST(asPandas=True)\n",
    "print(sst_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile Plotting Fun\n",
    "\n",
    "Since a pandas dataframe is a native output, quicklook plots are very easy to obtain as are output to csv for other software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "try:\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    ax1 = plt.subplot(1, 1, 1)\n",
    "    pd.plotting.scatter_matrix(pro_data, ax=ax1)\n",
    "    fig.savefig( '..//results/'+ year + '/' + instid + '/' + year + '/' + instid +'_profile_data.facetgrid.png', dpi=300, tight_layout=True)\n",
    "\n",
    "except:\n",
    "    print('no profile data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fig = plt.figure(1,figsize=(9,5))\n",
    "    ### CTD Temperature\n",
    "    ax1 = plt.subplot(1, 3, 1)\n",
    "    plt.plot(pro_data.ttp_temp,pro_data.pressure*10,'r',linewidth=.5)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.set_xlabel('Temperature')\n",
    "    ax1.set_ylabel('Depth (m)')\n",
    "    ### CTD PAR\n",
    "    ax1 = plt.subplot(1, 3, 2)\n",
    "    plt.plot(pro_data.par,pro_data.pressure*10,'y',linewidth=.5)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.set_xlabel('PAR')\n",
    "    ### CTD Temperature\n",
    "    ax1 = plt.subplot(1, 3, 3)\n",
    "    plt.plot(pro_data.fluor,pro_data.pressure*10,'g',linewidth=.5)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.set_xlabel('Chlor')\n",
    "\n",
    "    fig.savefig( '..//results/'+ year + '/' + instid + '/'+ year + '/' + instid +'_pro_data.simple.png',dpi=300)\n",
    "\n",
    "except:\n",
    "    print('no profile data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert elapse time to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time functions are in netcdf4 library\n",
    "try:\n",
    "    pro_data['datetime'] =[num2date(x['time'],'seconds since 1970-1-1') for i,x in pro_data.iterrows()]\n",
    "    \n",
    "except:\n",
    "    print('no profile data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profile data spells out the profile start time which is used to create the \"datetime\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_data['datetime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottom and Ice data start marking time from the \"Unit_Start_Time\" and \"Unit_Release_Time\" which should be incorporated into the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yaml.dump(IDNUMBER.config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_data['datetime'] =[num2date(x['time'],'seconds since '+ str(IDNUMBER.config['Unit_Start_Time']) ) for i,x in bottom_data.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trim out deck data using deploy date in config file for output csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_data = bottom_data.drop(bottom_data[bottom_data['datetime'] < IDNUMBER.config['Unit_Deploy_Time']].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_data['datetime'] =[num2date(x['time'],'seconds since '+ str(IDNUMBER.config['Unit_Release_Time']) ) for i,x in ice_data.iterrows()]\n",
    "ice_data['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sst_data['datetime'] =[num2date(x['time'],'seconds since 1970-1-1') for i,x in sst_data.iterrows()]\n",
    "    sst_data['datetime']\n",
    "except:\n",
    "    print(\"No SST data\")\n",
    "sst_data['datetime']\n",
    "    \n",
    "#filter out any dates that are before the deployment date\n",
    "#sst_data = sst_data[sst_data.datetime>IDNUMBER.config['Unit_Start_Time']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Data to be used in other software\n",
    "\n",
    "simple as sending the pandas dataframe to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add id,lat,lon from config to datafiles\n",
    "try:\n",
    "    ice_data['id'] = IDNUMBER.config['UnitID']\n",
    "    ice_data['dep_lat'] = IDNUMBER.config['Deploy_LatN']\n",
    "    ice_data['dep_lon'] = IDNUMBER.config['Deploy_LonW']\n",
    "    ice_data.to_csv( '..//results/'+instid + '/'+year + '/' + instid+'_ice_data.csv')\n",
    "except:\n",
    "    print(\"No under Ice data\")\n",
    "try:\n",
    "    sst_data['id'] = IDNUMBER.config['UnitID']\n",
    "    sst_data['dep_lat'] = IDNUMBER.config['Deploy_LatN']\n",
    "    sst_data['dep_lon'] = IDNUMBER.config['Deploy_LonW']\n",
    "    sst_data.to_csv( '..//results/'+instid + '/'+year + '/' + instid+'_sst_data.csv')\n",
    "except:\n",
    "    print(\"No SST data\")\n",
    "try:\n",
    "    bottom_data['id'] = IDNUMBER.config['UnitID']\n",
    "    bottom_data['dep_lat'] = IDNUMBER.config['Deploy_LatN']\n",
    "    bottom_data['dep_lon'] = IDNUMBER.config['Deploy_LonW']\n",
    "    bottom_data.to_csv( '..//results/'+instid + '/'+year + '/' + instid+'_bottom_data.csv')\n",
    "except:\n",
    "    print(\"No bottom data\")  \n",
    "try:\n",
    "    pro_data['id'] = IDNUMBER.config['UnitID']\n",
    "    pro_data['dep_lat'] = IDNUMBER.config['Deploy_LatN']\n",
    "    pro_data['dep_lon'] = IDNUMBER.config['Deploy_LonW']\n",
    "    pro_data.to_csv( '..//results/'+instid + '/'+year + '/' + instid+'_profile_data.csv')\n",
    "except:\n",
    "    print(\"No profile data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeseries Plots of Bottom/Ice/SST\n",
    "\n",
    "*Bottom with Chlor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1,figsize=(8,2.5))\n",
    "### CTD Temperature\n",
    "ax1a = plt.subplot(3, 1, 1)\n",
    "plt.plot(bottom_data.datetime,bottom_data.ttp_temp,'r',linewidth=.75)\n",
    "plt.plot(bottom_data.datetime,bottom_data.sst_temp,'m',linewidth=.5)\n",
    "ax1a.set_xticklabels([])\n",
    "ax1a.set_ylabel('Temp')\n",
    "### Par\n",
    "ax1b = plt.subplot(3, 1, 2)\n",
    "plt.plot(bottom_data.datetime,bottom_data.par,'y',linewidth=.75)\n",
    "ax1b.set_xticklabels([])\n",
    "ax1b.set_ylabel('PAR')\n",
    "### Chlor\n",
    "ax1c = plt.subplot(3, 1, 3)\n",
    "plt.plot(bottom_data.datetime,bottom_data.fluor,'g',linewidth=.75)\n",
    "ax1c.set_ylabel('Chlor')\n",
    "\n",
    "\n",
    "## specify datelabels and ticks for all three pannels\n",
    "ax1a.yaxis.set_minor_locator(ticker.MultipleLocator(1))\n",
    "ax1a.xaxis.set_major_locator(DayLocator(bymonthday=1))\n",
    "ax1a.xaxis.set_minor_locator(DayLocator(bymonthday=15))\n",
    "#ax1b.yaxis.set_minor_locator(ticker.MultipleLocator(2.5))\n",
    "ax1b.xaxis.set_major_locator(DayLocator(bymonthday=1))\n",
    "ax1b.xaxis.set_minor_locator(DayLocator(bymonthday=15))\n",
    "\n",
    "ax1c.tick_params(labelbottom=True,labeltop=False)\n",
    "#ax1c.yaxis.set_minor_locator(ticker.MultipleLocator(2.5))\n",
    "ax1c.xaxis.set_major_locator(DayLocator(bymonthday=1))\n",
    "ax1c.xaxis.set_minor_locator(DayLocator(bymonthday=15))\n",
    "ax1c.xaxis.set_major_formatter(ticker.NullFormatter())\n",
    "ax1c.xaxis.set_minor_formatter(DateFormatter('%b %y'))\n",
    "ax1c.xaxis.set_major_formatter(DateFormatter(''))\n",
    "ax1c.xaxis.set_tick_params(which='major', pad=15)\n",
    "ax1c.xaxis.set_tick_params(which='minor', pad=5)\n",
    "\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "fig.savefig( '..//results/'+instid + '/'+year + '/' + instid+'_chlor_bottom_data.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bottom Temp Detail*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.plot(bottom_data.datetime,bottom_data.ttp_temp,'r',linewidth=.75, label='ttp fast response temp')\n",
    "plt.plot(bottom_data.datetime,bottom_data.sst_temp,'b',linewidth=.5, label='sst slow response temp')\n",
    "plt.ylabel('Temp C')\n",
    "plt.xlabel('Date')\n",
    "plt.title(instid)\n",
    "plt.xticks(rotation=30)\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.savefig( '..//results/'+instid + '/'+year + '/' + instid+'_bottom_data.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under Ice\n",
    "*Ice with Chlor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1,figsize=(8,2.5))\n",
    "### CTD Temperature\n",
    "ax1a = plt.subplot(3, 1, 1)\n",
    "plt.plot(ice_data.datetime,ice_data.ttp_temp,'r',linewidth=.75)\n",
    "plt.plot(ice_data.datetime,ice_data.sst_temp,'m',linewidth=.5)\n",
    "ax1a.set_xticklabels([])\n",
    "ax1a.set_ylabel('Temp')\n",
    "### Par\n",
    "ax1b = plt.subplot(3, 1, 2)\n",
    "plt.plot(ice_data.datetime,ice_data.par,'y',linewidth=.75)\n",
    "ax1b.set_xticklabels([])\n",
    "ax1b.set_ylabel('PAR')\n",
    "### Chlor\n",
    "ax1c = plt.subplot(3, 1, 3)\n",
    "plt.plot(ice_data.datetime,ice_data.fluor,'g',linewidth=.75)\n",
    "ax1c.set_ylabel('Chlor')\n",
    "\n",
    "## specify datelabels and ticks for all three pannels\n",
    "ax1a.yaxis.set_minor_locator(ticker.MultipleLocator(2.5))\n",
    "ax1a.xaxis.set_major_locator(DayLocator(bymonthday=1))\n",
    "ax1a.xaxis.set_minor_locator(DayLocator(bymonthday=15))\n",
    "ax1b.yaxis.set_minor_locator(ticker.MultipleLocator(2.5))\n",
    "ax1b.xaxis.set_major_locator(DayLocator(bymonthday=1))\n",
    "ax1b.xaxis.set_minor_locator(DayLocator(bymonthday=15))\n",
    "\n",
    "ax1c.tick_params(labelbottom=True,labeltop=False)\n",
    "ax1c.yaxis.set_minor_locator(ticker.MultipleLocator(2.5))\n",
    "ax1c.xaxis.set_major_locator(DayLocator(bymonthday=1))\n",
    "ax1c.xaxis.set_minor_locator(DayLocator(bymonthday=15))\n",
    "ax1c.xaxis.set_major_formatter(ticker.NullFormatter())\n",
    "ax1c.xaxis.set_minor_formatter(DateFormatter('%b %y'))\n",
    "ax1c.xaxis.set_major_formatter(DateFormatter(''))\n",
    "ax1c.xaxis.set_tick_params(which='major', pad=15)\n",
    "ax1c.xaxis.set_tick_params(which='minor', pad=5)\n",
    "\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "fig.savefig( '..//results/'+instid + '/'+year + '/' + instid+'_chlorpar_ice_data.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ice no Chlor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1,figsize=(8,2.5))\n",
    "### CTD Temperature\n",
    "ax1a = plt.subplot(3, 1, 1)\n",
    "plt.plot(ice_data.datetime,ice_data.ttp_temp,'r',linewidth=.75)\n",
    "plt.plot(ice_data.datetime,ice_data.sst_temp,'m',linewidth=.5)\n",
    "ax1a.set_xticklabels([])\n",
    "ax1a.set_ylabel('Temp')\n",
    "\n",
    "## specify datelabels and ticks for all three pannels\n",
    "ax1a.yaxis.set_minor_locator(ticker.MultipleLocator(2.5))\n",
    "ax1a.xaxis.set_major_locator(DayLocator(bymonthday=1))\n",
    "ax1a.xaxis.set_minor_locator(DayLocator(bymonthday=15))\n",
    "ax1b.yaxis.set_minor_locator(ticker.MultipleLocator(2.5))\n",
    "ax1b.xaxis.set_major_locator(DayLocator(bymonthday=1))\n",
    "ax1b.xaxis.set_minor_locator(DayLocator(bymonthday=15))\n",
    "\n",
    "ax1b.tick_params(labelbottom=True,labeltop=False)\n",
    "ax1b.yaxis.set_minor_locator(ticker.MultipleLocator(2.5))\n",
    "ax1b.xaxis.set_major_locator(DayLocator(bymonthday=1))\n",
    "ax1b.xaxis.set_minor_locator(DayLocator(bymonthday=15))\n",
    "ax1b.xaxis.set_major_formatter(ticker.NullFormatter())\n",
    "ax1b.xaxis.set_minor_formatter(DateFormatter('%b %y'))\n",
    "ax1b.xaxis.set_major_formatter(DateFormatter(''))\n",
    "ax1b.xaxis.set_tick_params(which='major', pad=15)\n",
    "ax1b.xaxis.set_tick_params(which='minor', pad=5)\n",
    "\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "fig.savefig( '..//results/'+instid + '/'+year + '/' + instid+'_ice_data.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea Surface Drifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fig = plt.figure(1,figsize=(8,2.5))\n",
    "    ### CTD Temperature\n",
    "    ax1a = plt.subplot(3, 1, 1)\n",
    "    plt.plot(sst_data.datetime,sst_data.ttp_temp,'r.',markersize=.5,linewidth=.75)\n",
    "    plt.plot(sst_data.datetime,sst_data.sst_temp,'m.',markersize=.5,linewidth=.5)\n",
    "    ax1a.set_xticklabels([])\n",
    "    ax1a.set_ylabel('Temp')\n",
    "    ### Par\n",
    "    ax1b = plt.subplot(3, 1, 2)\n",
    "    plt.plot(sst_data.datetime,sst_data.par,'y.',markersize=.5,linewidth=.75)\n",
    "    ax1b.set_xticklabels([])\n",
    "    ax1b.set_ylabel('PAR')\n",
    "    ### Chlor\n",
    "    ax1c = plt.subplot(3, 1, 3)\n",
    "    plt.plot(sst_data.datetime,sst_data.fluor,'g.',markersize=.5,linewidth=.75)\n",
    "    ax1c.set_ylabel('Chlor')\n",
    "\n",
    "\n",
    "    ## specify datelabels and ticks for all three pannels\n",
    "    ax1a.yaxis.set_minor_locator(ticker.MultipleLocator(2.5))\n",
    "    ax1a.xaxis.set_major_locator(DayLocator(bymonthday=1))\n",
    "    ax1a.xaxis.set_minor_locator(DayLocator(bymonthday=15))\n",
    "    ax1b.yaxis.set_minor_locator(ticker.MultipleLocator(2.5))\n",
    "    ax1b.xaxis.set_major_locator(DayLocator(bymonthday=1))\n",
    "    ax1b.xaxis.set_minor_locator(DayLocator(bymonthday=15))\n",
    "\n",
    "    ax1c.tick_params(labelbottom=True,labeltop=False)\n",
    "    ax1c.yaxis.set_minor_locator(ticker.MultipleLocator(2.5))\n",
    "    ax1c.xaxis.set_major_locator(DayLocator(bymonthday=1))\n",
    "    ax1c.xaxis.set_minor_locator(DayLocator(bymonthday=15))\n",
    "    ax1c.xaxis.set_major_formatter(ticker.NullFormatter())\n",
    "    ax1c.xaxis.set_minor_formatter(DateFormatter('%b %y'))\n",
    "    ax1c.xaxis.set_major_formatter(DateFormatter(''))\n",
    "    ax1c.xaxis.set_tick_params(which='major', pad=15)\n",
    "    ax1c.xaxis.set_tick_params(which='minor', pad=5)\n",
    "\n",
    "    fig.savefig( '..//results/'+instid + '/'+year + '/' + instid+'_sst_data.png',dpi=300, tight_layout=True)\n",
    "except:\n",
    "    print(\"No SST Data yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
