{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code and Documentation to Decode Popup Buoy Transmitted/Recorded Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Requirements\n",
    "\n",
    "Developed/tested for:\n",
    "- python >=3.6 \n",
    "\n",
    "requires:\n",
    "- pyyaml >= 3.13 \n",
    "- pandas >= 0.23.4\n",
    "- numpy >= 1.15.4\n",
    "- matplotlib >= 3.0.2\n",
    "- jupyterlab >= 1.0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import collections\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import YearLocator, WeekdayLocator, MonthLocator, DayLocator, HourLocator, DateFormatter\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import NullFormatter  # useful for `logit` scale\n",
    "\n",
    "import datetime\n",
    "from netCDF4 import num2date, date2num\n",
    "\n",
    "import cftime\n",
    "# from nc_time_axis import CalendarDateTime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure\n",
    "\n",
    "Data downloaded from Popup Buoy's directly, generate 8 binary files. \n",
    "Data stitched from SBD messages should generate the same files, assuming full transmission. Often partial transmission means fewer complete files. \n",
    "\n",
    "Filenames:\n",
    "- BOTDAT.TXT\n",
    "- FILEPOS.TXT\n",
    "- ICEDAT.TXT\n",
    "- JPGxxxxx.JPG\n",
    "- PRODAT.TXT\n",
    "- SSTDAT.TXT\n",
    "- SUMMARY.TXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A list of pop-up units that transmitted data from the field\n",
    "#### *year deployed* - *imei number* - *site: SBD Active/Inactive* \n",
    " - 2018 - 300434063921240 - C2: Inactive  <br>\n",
    " - 2018 - 300434063823800 - C10/C11: Inactive  <br>\n",
    " - 2018 - 300434063928220 - C12: Inactive  <br>\n",
    " - 2018 - 300434063925210 - M5: Inactive  <br>\n",
    " <br>\n",
    " - 2019 - 300434063470010 - S.W. of M5: Inative  <br>\n",
    " - 2019 - 300434063477010 - N.W. of M5: Inactive  <br>\n",
    " - 2019 - 300434063861360 - N.E. of Saint Lawrence: Inactive  <br>\n",
    " - 2019 - 300434063474010 - M8 PopTop: Inactive  <br>\n",
    " <br>\n",
    " - 2020 - 300434063479200 - C2: Active  <br>\n",
    " - 2020 - 300434063924230 - C12: Active  <br>\n",
    "  <br>\n",
    " - 2020 - 300434063863550\n",
    " - 2020 - 300434063471670\n",
    " - 2020 - 300434063473020\n",
    " - 2020 - 300434063471040\n",
    " - 2020 - 300434063472040\n",
    " - 2020 - 300434063862560\n",
    " - 2020 - 300434063472010\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['300434063477010', '300434063473020', '.DS_Store', '300434063479200', '300434063472010', '300434063479010', '300434063471040', '300434063924230', '300434063862560', '300434063863550', '300434063921240', '300434063471670', '300434063470010', '300434063861360', '300434063474010']\n"
     ]
    }
   ],
   "source": [
    "# identify the folder that holds the raw data as data_dir\n",
    "data_dir = os.path.join(\"..\", \"rawdata\", \"ecofoci.popupsbd\")\n",
    "\n",
    "# print out the serial numbers of the units that have folders in the rawdata folder\n",
    "id_dir = os.listdir(data_dir)\n",
    "print(id_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>INPUT:</b> COPY THE IMEI OF INTEREST FROM ABOVE AND PASTE IT IN THE instid = ['imei#'] BELOW. INPUT THE LAST 4 DIGITS OF THE IMEI sn = ['last4imei'] AND THE YEAR THE FLOAT WAS DEPLOYED. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *INPUT*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../results/2020/300434063472030'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#id_dir=['300434063863550','300434063471670','300434063473020','300434063471040','300434063472040','300434063862560','300434063472010', '300434063764860', '300434063472030']\n",
    "\n",
    "instid = '300434063472030' # INPUT unit imei number\n",
    "sn = instid[-4:] # INPUT last 4 digits of imei\n",
    "year = '2020' # INPUT year of deployment \n",
    "\n",
    "#establish the root path to the unit's results folder\n",
    "root_path = os.path.join('..', 'results', year, instid)\n",
    "root_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *RUN SCRIPT*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../yaml/300434063472030.yaml\n"
     ]
    }
   ],
   "source": [
    "# This is a dictionary that links a key to a value\n",
    "# Here we link the raw hexadecimal files located in the root path to the names on the left of the semicolon\n",
    "instfile_dic = {'bot_file': root_path +'/BOTDAT.TXT',\n",
    "                'ice_file': root_path +'/ICEDAT.TXT',\n",
    "                'pro_file': root_path +'/PRODAT.TXT',\n",
    "                'sst_file': root_path +'/SSTDAT.TXT'}\n",
    "\n",
    "# In order to transalate the data we need a configuration file\n",
    "# This line of code points to where the .yaml configuration file is and converts it to a string\n",
    "# The code will use any file located in the root path that has a '####.yaml' ending\n",
    "instconfig = glob.glob(os.path.join('..', 'yaml', instid+'.yaml'), recursive=False)\n",
    "i_config = ''.join(map(str, instconfig))\n",
    "print(i_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each data file, we are going to read in the entire file and convert the binary to hex.  There are multiple line, lengths we are going to have to address but the start of each record is denoted by 'FFFF'.  We can split the filestring on this parameter but we need to be aware of 'FFFFF' or 'FFFFFF' posibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Basic Approach***\n",
    "\n",
    "The two modules below will allow for a simple readin of the file for very simple analysis and debugging... the code of consequence that involves conversion of measurements from engineering units to science units as all defined in the PopUpBuoy CLASS in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HexView(file):\n",
    "    with open(file, 'rb') as in_file:\n",
    "        while True:\n",
    "            hexdata = in_file.read().hex().upper()     # Read the shortest possible line\n",
    "            if len(hexdata) == 0:                      # breaks loop once no more binary data is read\n",
    "                break\n",
    "            \n",
    "            return(hexdata.upper())\n",
    "        \n",
    "def HexSplit(hexstr):\n",
    "    if hexstr.find('FFFFF') == -1:\n",
    "        print(\"No FFFFF, proceed to split on FFFF\")\n",
    "        sample_raw = hexstr.split('FFFF')[1:]\n",
    "    else:\n",
    "        print('FFFFF found')\n",
    "        #this puts in the proper line endings but removes a variable \n",
    "        #   F from the end of each string.  Add the F string back\n",
    "        sample_raw = []\n",
    "        for substr in hexstr.split('FFFFF')[1:]: \n",
    "            sample_raw = sample_raw + (substr + 'F').split('FFFF')\n",
    "\n",
    "        sample_raw[-1] = sample_raw[-1][:-1]\n",
    "        \n",
    "    return(sample_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../results/2020/300434063472030/BOTDAT.TXT'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vv/blq095kj0xj9nz1v0ffdwfch0000gp/T/ipykernel_19948/1570655672.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mactive_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstfile_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bot_file'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhexstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHexView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msample_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHexSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhexstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/vv/blq095kj0xj9nz1v0ffdwfch0000gp/T/ipykernel_19948/3746932519.py\u001b[0m in \u001b[0;36mHexView\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mHexView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0min_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mhexdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# Read the shortest possible line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhexdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m                      \u001b[0;31m# breaks loop once no more binary data is read\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../results/2020/300434063472030/BOTDAT.TXT'"
     ]
    }
   ],
   "source": [
    "active_file = instfile_dic['bot_file']\n",
    "\n",
    "hexstr = HexView(active_file)\n",
    "sample_raw = HexSplit(hexstr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class description and routine code\n",
    "\n",
    "## Decode sample data for each file type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottom Data (BOTDAT.TXT) / Under Ice Data (ICEDAT.TXT)\n",
    "\n",
    "This data has two record lengths.  17 and 19.  Since we've split on 'FFFF' and broken the record up into samples, the initial 2bytes are no longer in the record so the record lengths are now 15 and 17 (which is a string length of 30 and 34 characters)\n",
    "\n",
    "***MSG Decode Key***\n",
    "![BotDecodeMsg](../documentation/decode_images/BotDat_msg_decode.png)\n",
    "\n",
    "***Engineering to Science Conversions***\n",
    "![BotDecodeMsg](../documentation/decode_images/BotDat_msg_cal.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Topside Temp was changed to fr_temp for \"Fast Response\" temperature probe. This probe can be oriented directly under melting sea ice in the Under Ice phase (ICEDAT.TXT)<br>\n",
    "    Underside Temp was changed to sr_temp for \"Slow Response\" temperature probe. The fr_temp and sr_temp should track together in the Bottom Phase (BOTDAT.TXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile Data (PRODAT.TXT) \n",
    "\n",
    "This data has two record lengths.  13 and 15.  Since we've split on 'FFFF' and broken the record up into samples, the initial 2bytes are no longer in the record so the record lengths are now 11 and 13 (which is a string length of 26 and 30 characters).  This file does not have the bottom temp or the reference temp fields.\n",
    "\n",
    "***MSG Decode Key***\n",
    "![ProDecodeMsg](../documentation/decode_images/ProDat_msg_decode.png)\n",
    "\n",
    "***Engineering to Science Conversions***\n",
    "![ProCalMsg](../documentation/decode_images/ProDat_msg_cal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Topside Temp was changed to fr_temp for \"Fast Response\" temperature probe. This temp probe is the only one sampling during the profile.  <br>\n",
    "    Underside Temp was changed to sr_temp for \"Slow Response\" temperature probe. This probe does not sample during the profile because it doesn't have a fast response time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SST Data (SSTDAT.TXT) \n",
    "\n",
    "This data has eight record lengths.  17 and 35, 18 and 36, 19 and 37 or 20 and 38.  Since we've split on 'FFFF' and broken the record up into samples, the initial 2bytes are no longer in the record so the record lengths are now 15 and 33, 16 and 34, 17 and 35 or 18 and 36.  The 8 types of files are if there is the short time format or the long time format, if TTS is provided or not, and if there is gps encoding or not for each of the time formats.\n",
    "\n",
    "17: short-time, no gps, no TTS   \n",
    "18: short-time, no gps, TTS   \n",
    "19: long-time, no gps, no TTS   \n",
    "20: long-time, no gps, TTS   \n",
    "35: short-time, gps, no TTS   \n",
    "36: short-time, gps, TTS   \n",
    "37: long-time, gps, no TTS   \n",
    "38: long-time, gps, TTS   \n",
    "\n",
    "**UPDATE 3-12-2019:**  TTF is also an option now as an additional record of 2 bytes\n",
    "\n",
    "***MSG Decode Key***\n",
    "![SSTDecodeMsg](../documentation/decode_images/SSTDat_msg_decode.png)\n",
    "\n",
    "***Engineering to Science Conversions***\n",
    "![SSTDecodeMsg](../documentation/decode_images/SSTDat_msg_cal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Topside Temp was changed to fr_temp for \"Fast Response\" temperature probe. This probe is out of the water in the Sea Surface Drifter phase (SSTDAT.TXT) <br>\n",
    "    Underside Temp was changed to sr_temp for \"Slow Response\" temperature probe. This probe lies 8 inches below the surface in the Sea Surface Drifter phase (SSTDAT.TXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the PopUpBuoy Class\n",
    "\n",
    "import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preload matplotlib plot parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### specify primary bulk figure parameters\n",
    "fontsize = 10\n",
    "labelsize = 10\n",
    "plotstyle = 'seaborn'\n",
    "max_xticks = 10\n",
    "plt.style.use('seaborn-ticks')\n",
    "mpl.rcParams['svg.fonttype'] = 'none'\n",
    "mpl.rcParams['ps.fonttype'] = 42 #truetype/type2 fonts instead of type3\n",
    "mpl.rcParams['pdf.fonttype'] = 42 #truetype/type2 fonts instead of type3\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "mpl.rcParams['axes.edgecolor'] = 'black'\n",
    "mpl.rcParams['axes.linewidth'] = 1.5\n",
    "mpl.rcParams['axes.labelcolor'] = 'black'\n",
    "mpl.rcParams['grid.linestyle'] = '--'\n",
    "mpl.rcParams['grid.linestyle'] = '--'\n",
    "mpl.rcParams['xtick.major.size'] = 4\n",
    "mpl.rcParams['xtick.minor.size'] = 2\n",
    "mpl.rcParams['xtick.major.width'] = 2\n",
    "mpl.rcParams['xtick.minor.width'] = 0.5\n",
    "mpl.rcParams['ytick.major.size'] = 4\n",
    "mpl.rcParams['ytick.minor.size'] = 2\n",
    "mpl.rcParams['ytick.major.width'] = 2\n",
    "mpl.rcParams['ytick.minor.width'] = 0.5\n",
    "mpl.rcParams['ytick.direction'] = 'out'\n",
    "mpl.rcParams['xtick.direction'] = 'out'\n",
    "mpl.rcParams['ytick.color'] = 'black'\n",
    "mpl.rcParams['xtick.color'] = 'black'\n",
    "\n",
    "mpl.rcParams['contour.negative_linestyle'] = 'solid'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def signed_int(hexstr):\n",
    "    '''function to convert hex string to signed int'''\n",
    "    s_int = int(hexstr,16)\n",
    "    if s_int >= 0x8000:\n",
    "        s_int -= 0x10000  \n",
    "\n",
    "    return(s_int)\n",
    "\n",
    "class PopUpBuoys(object):\n",
    "    \"\"\"Class definitions to read and Process PopUp Buoy Data Streams\"\"\"\n",
    "\n",
    "\n",
    "    active_stream = 'bottom'\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.instfile_dic = {'bottom': path + '/BOTDAT.TXT',\n",
    "                             'ice': path + '/ICEDAT.TXT',\n",
    "                             'profile': path + '/PRODAT.TXT',\n",
    "                             'sst': path + '/SSTDAT.TXT'}\n",
    "    \n",
    "    def LoadCoefs(self, config_file='default.yaml'):\n",
    "        ''' Load yaml formated config file '''\n",
    "        self.config = yaml.safe_load(open(config_file))\n",
    "\n",
    "    \n",
    "    def HexView(self, active_stream='', verbose=True):\n",
    "        '''\n",
    "        input: reference to proper filepointer, options are keys \n",
    "            to the self.instfile_dic dictionary\n",
    "        '''\n",
    "        if active_stream:\n",
    "            self.active_stream = active_stream\n",
    "            \n",
    "        file = self.instfile_dic[self.active_stream]\n",
    "        with open(file, 'rb') as in_file:\n",
    "            while True:\n",
    "                hexdata = in_file.read().hex().upper()     \n",
    "                if len(hexdata) == 0:                      \n",
    "                # breaks loop once no more binary data is read\n",
    "                    break\n",
    "                self.hexstr = hexdata.upper()\n",
    "                \n",
    "                if verbose:\n",
    "                    return(hexdata.upper())\n",
    "\n",
    "    def HexSplit(self, verbose=True):\n",
    "        '''\n",
    "        input: results of HexView (inherits output)\n",
    "        '''\n",
    "        if self.hexstr.find('FFFFF') == -1:\n",
    "            print(\"No FFFFF, proceed to split on FFFF\")\n",
    "            #because in this case the first index is empty\n",
    "            sample_raw = self.hexstr.split('FFFF')[1:] \n",
    "        else:\n",
    "            print('FFFFF found')\n",
    "            #this puts in the proper line endings but removes a variable \n",
    "            #   F from the end of each string.  Add the F string back\n",
    "            sample_raw = []\n",
    "            for substr in self.hexstr.split('FFFFF'): \n",
    "                sample_raw = sample_raw + (substr + 'F').split('FFFF')\n",
    "\n",
    "            sample_raw[-1] = sample_raw[-1][:-1]\n",
    "        \n",
    "        self.sample_raw = sample_raw\n",
    "        \n",
    "        if verbose:\n",
    "            return(sample_raw)\n",
    "    \n",
    "    def Bottom(self, asPandas=False):\n",
    "        ''' Bottom is equivalent to the TimeSeriesBase'''\n",
    "        \n",
    "        if self.active_stream != 'bottom':\n",
    "            print(\"current active file is {} - can't output bottom data\".format(self.active_stream))\n",
    "            return\n",
    "\n",
    "        data = self.TimeSeriesBase()\n",
    "\n",
    "        if asPandas:\n",
    "            data = pd.DataFrame.from_dict(data,orient='index')\n",
    "        return(data)\n",
    "    \n",
    "\n",
    "    def Profile(self, asPandas=False):\n",
    "        try:\n",
    "            self.sample_raw\n",
    "        except:\n",
    "            print(\"Run PopUpBuoys.HexView and PopUpBuoys.HexSplit First\")\n",
    "            return\n",
    "\n",
    "        if self.active_stream != 'profile':\n",
    "            print(\"current active file is {} - can't output profile data\".format(self.active_stream))\n",
    "            return\n",
    "                        \n",
    "        data=collections.OrderedDict()\n",
    "\n",
    "        for sample_num, sample in enumerate(self.sample_raw):\n",
    "                        \n",
    "            if len(sample) == 22: #2byte timeword\n",
    "                \n",
    "                # unlike sst, bottom, and ice - profiles don't need to be multiplied by a sample interval\n",
    "                #seconds since 1970-01-01\n",
    "                try:\n",
    "                    time = int(sample[0:4],16)/100 + profile_starttime\n",
    "                except:\n",
    "                    time = int(sample[0:4],16)/100 + 0\n",
    "                    \n",
    "                pressure =  self.PressureConversion(int(sample[4:8],16)) \n",
    "                \n",
    "                fr_temp_ADC = signed_int(sample[8:12])\n",
    "                fr_temp = self.TempConversion(engr_meas=fr_temp_ADC,\n",
    "                                                  coefA=self.config['fr_temp_cal']['Acoef'], \n",
    "                                                  coefB=self.config['fr_temp_cal']['Bcoef'],\n",
    "                                                  coefC=self.config['fr_temp_cal']['Ccoef'],\n",
    "                                                  version=2)\n",
    "\n",
    "                rawpvalue = signed_int(sample[12:16])                  \n",
    "                par = self.PARConversion(engr_meas=rawpvalue,\n",
    "                                        coef_offset=self.config['par_cal']['offset'], \n",
    "                                        coef_slope=self.config['par_cal']['slope'])   \n",
    "\n",
    "                rawfvalue = signed_int(sample[16:20])                   \n",
    "                fluor = self.FluorConversion(engr_meas=rawfvalue,\n",
    "                                        coef_offset=self.config['fluor_cal']['offset'], \n",
    "                                        coef_slope=self.config['fluor_cal']['slope'])  \n",
    "\n",
    "                tilt = int(sample[20:22],16) #degrees\n",
    "                \n",
    "            elif len(sample) == 26: #4byte timeword, reserved for profile starttime\n",
    "                \n",
    "                #seconds since 1970-01-01\n",
    "                profile_starttime = int(sample[0:8],16)\n",
    "                \n",
    "                time = profile_starttime\n",
    "                \n",
    "                pressure =  self.PressureConversion(int(sample[8:12],16))  \n",
    "                \n",
    "                fr_temp_ADC = signed_int(sample[12:16])\n",
    "                fr_temp = self.TempConversion(engr_meas=fr_temp_ADC,\n",
    "                                                  coefA=self.config['fr_temp_cal']['Acoef'], \n",
    "                                                  coefB=self.config['fr_temp_cal']['Bcoef'],\n",
    "                                                  coefC=self.config['fr_temp_cal']['Ccoef'],\n",
    "                                                  version=2)\n",
    "                \n",
    "                rawpvalue = signed_int(sample[16:20])             \n",
    "                par = self.PARConversion(engr_meas=rawpvalue,\n",
    "                                        coef_offset=self.config['par_cal']['offset'], \n",
    "                                        coef_slope=self.config['par_cal']['slope'])  \n",
    "\n",
    "                rawfvalue = signed_int(sample[20:24])                \n",
    "                fluor = self.FluorConversion(engr_meas=rawfvalue,\n",
    "                                        coef_offset=self.config['fluor_cal']['offset'], \n",
    "                                        coef_slope=self.config['fluor_cal']['slope'])  \n",
    "\n",
    "                tilt = int(sample[24:26],16) #degrees\n",
    "            \n",
    "            elif len(sample) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                continue # skip lines with too many data points\n",
    "            #save to dictionary\n",
    "            data[sample_num] = {'time':time,\n",
    "                                'pressure':pressure,\n",
    "                                'fr_temp':fr_temp,\n",
    "                                'fr_temp_ADC':fr_temp_ADC,\n",
    "                                'par':par,\n",
    "                                'fluor':fluor,\n",
    "                                'tilt':tilt}        \n",
    "        \n",
    "        if asPandas:\n",
    "            data = pd.DataFrame.from_dict(data,orient='index')\n",
    "        return(data)\n",
    "        \n",
    "    def Ice(self, asPandas=False):\n",
    "        ''' Ice is equivalent to the TimeSeriesBase'''\n",
    "\n",
    "        if self.active_stream != 'ice':\n",
    "            print(\"current active file is {} - can't output ice data\".format(self.active_stream))\n",
    "            return\n",
    "\n",
    "        data = self.TimeSeriesBase()\n",
    "\n",
    "        if asPandas:\n",
    "            data = pd.DataFrame.from_dict(data,orient='index')\n",
    "        return(data)\n",
    "    \n",
    "    def SST(self, asPandas=False):\n",
    "        ''' SST is equivalent to the TimeSeriesBase + GPS information'''\n",
    "        \n",
    "        if self.active_stream != 'sst':\n",
    "            print(\"current active file is {} - can't output sst data\".format(self.active_stream))\n",
    "            return\n",
    "\n",
    "        data = self.TimeSeriesBase()\n",
    "\n",
    "\n",
    "        for sample_num, sample in enumerate(self.sample_raw):\n",
    "\n",
    "            if (len(sample) == 30) or (len(sample) == 66): #2byte timeword, no TTS (w and w/o gps)\n",
    "                TTS = np.nan\n",
    "                if len(sample) == 66:\n",
    "                    print(\"analyze GPS\")\n",
    "                    gps_data = self.GPSSeriesBase(sample_num, sample, empty=False)\n",
    "                else:\n",
    "                    gps_data = self.GPSSeriesBase(sample_num, sample, empty=True)\n",
    "\n",
    "            elif (len(sample) == 32) or (len(sample) == 68): #2byte timeword, yes TTS (w and w/o gps)\n",
    "                TTS = int(sample[30:32],16)\n",
    "                if len(sample) == 68:\n",
    "                    gps_data = self.GPSSeriesBase(sample_num, sample, offset=2, empty=False)\n",
    "                else:\n",
    "                    gps_data = self.GPSSeriesBase(sample_num, sample, empty=True)\n",
    "\n",
    "            elif (len(sample) == 34) or (len(sample) == 70): #4byte timeword, no TTS (w and w/o gps)\n",
    "                TTS = np.nan\n",
    "                if len(sample) == 70:\n",
    "                    gps_data = self.GPSSeriesBase(sample_num, sample, offset=4, empty=False)\n",
    "                else:\n",
    "                    gps_data = self.GPSSeriesBase(sample_num, sample, empty=True)\n",
    "\n",
    "            elif (len(sample) == 36) or (len(sample) == 72): #4byte timeword, yes TTS (w and w/o gps)\n",
    "                TTS = int(sample[30:32],16)\n",
    "                if len(sample) == 72:\n",
    "                    gps_data = self.GPSSeriesBase(sample_num, sample, offset=6, empty=False)\n",
    "                else:\n",
    "                    gps_data = self.GPSSeriesBase(sample_num, sample, empty=True)\n",
    "\n",
    "            elif len(sample) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                continue # skip lines with too many data points\n",
    "            #save to dictionary\n",
    "\n",
    "            try:\n",
    "                data[sample_num].update({'TTS':TTS})        \n",
    "                data[sample_num].update(gps_data[sample_num])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if asPandas:\n",
    "            return(pd.DataFrame.from_dict(data,orient='index'))\n",
    "        else:\n",
    "            return(data)\n",
    "\n",
    "\n",
    "    ### The folowing BASE functions are for convenience for reading and coding.  All redundant\n",
    "    # pattern reads are below.  The only challenge is that each BASE function reads the entire hex\n",
    "    # string (non-issue for files of size we expect)\n",
    "    def GPSSeriesBase(self,sample_num, sample, offset=0, empty=True):\n",
    "        '''SST only, the last 18 bytes are the same format:\n",
    "         GPSLat, GPSLon, GPSDate, GPSTime, TTF, Max_Tilt\n",
    "         Regardless of whether TTS or longdates are used.  This subroutine returns the \n",
    "         GPS dictionary based on a byte offset given the record length\n",
    "\n",
    "         Passing empty=True sends missing data back for GPS Data'''\n",
    "        data=collections.OrderedDict()\n",
    "\n",
    "        if not empty:\n",
    "            GPSLat = int(sample[30+offset:38+offset],16)/1000000\n",
    "            GPSLon = (int(sample[38+offset:46+offset],16)- 4294967295) / 1000000\n",
    "            GPSDate = int(sample[46+offset:54+offset],16)\n",
    "            GPSTime = int(sample[54+offset:62+offset],16)\n",
    "            TTF = int(sample[62+offset:64+offset],16)\n",
    "            Max_Tilt = int(sample[62+offset:66+offset],16)   \n",
    "\n",
    "        else:\n",
    "            GPSLat = GPSLon = GPSDate = GPSTime = TTF = Max_Tilt = np.nan \n",
    "            \n",
    "        #save to dictionary\n",
    "        data[sample_num] = {'GPSLat':GPSLat,\n",
    "                            'GPSLon':GPSLon,\n",
    "                            'GPSDate':GPSDate,\n",
    "                            'GPSTime':GPSTime,\n",
    "                            'TTF':TTF,\n",
    "                            'Max_Tilt':Max_Tilt}  \n",
    "            \n",
    "        return(data)   \n",
    "\n",
    "    def TimeSeriesBase(self):\n",
    "        '''Bottom, Ice, and SST all have the same base transmission information,\n",
    "            e.g. the first 17 bytes (short time stamp) / 19 bytes (long time stamp)\n",
    "            are the same.  Each of the the appropriate modules will call this communal module\n",
    "            first.\n",
    "\n",
    "            Bottom and Ice don't report any additional information beyond the base info so \n",
    "            they are essential decorators/wrappers for this function\n",
    "\n",
    "            Returns: Dictionary'''\n",
    "        try:\n",
    "            self.sample_raw\n",
    "        except:\n",
    "            print(\"Run PopUpBuoys.HexView and PopUpBuoys.HexSplit First\")\n",
    "            return\n",
    "\n",
    "        data=collections.OrderedDict()\n",
    "\n",
    "        for sample_num, sample in enumerate(self.sample_raw):\n",
    "            \n",
    "            #record length conditionals are due to number of varying outputs\n",
    "            #SST dominates the number of options due to:\n",
    "            # 2byte timeword, no TTS, no GPS (30)\n",
    "            # 4byte timeword, yes TTS, yes GPS (72)\n",
    "            #  and every permutation of the three functions\n",
    "            if ((len(sample) == 30) or (len(sample) == 32) or \n",
    "                            (len(sample) == 60) or (len(sample) == 64) or (len(sample) == 66) or (len(sample) == 68)) : #2byte timeword\n",
    "                \n",
    "                #seconds since 1970-01-01\n",
    "                if self.active_stream in ['sst']:\n",
    "                    #the correction is available for any non-utc setup pc\n",
    "                    time_base = date2num(datetime.datetime.strptime(IDNUMBER.config['Unit_Release_Time'],\n",
    "                                        '%Y-%m-%d %H:%M:%S'),\n",
    "                                                                    'seconds since 1970-1-1')\n",
    "                else:\n",
    "                    time_base = 0 \n",
    "\n",
    "                time = int(sample[0:4],16) * self.config['sample_interval'][self.active_stream] + time_base\n",
    "                \n",
    "                pressure =  self.PressureConversion(int(sample[4:8],16))  \n",
    "                \n",
    "                fr_temp_ADC = signed_int(sample[8:12])\n",
    "                fr_temp = self.TempConversion(engr_meas=fr_temp_ADC,\n",
    "                                                  coefA=self.config['fr_temp_cal']['Acoef'], \n",
    "                                                  coefB=self.config['fr_temp_cal']['Bcoef'],\n",
    "                                                  coefC=self.config['fr_temp_cal']['Ccoef'],\n",
    "                                                  version=2)\n",
    "\n",
    "                sr_temp_ADC = signed_int(sample[12:16])\n",
    "                sr_temp = self.TempConversion(engr_meas=sr_temp_ADC,\n",
    "                                                  coefA=self.config['sr_temp_cal']['Acoef'], \n",
    "                                                  coefB=self.config['sr_temp_cal']['Bcoef'],\n",
    "                                                  coefC=self.config['sr_temp_cal']['Ccoef'],\n",
    "                                                  version=2)\n",
    "\n",
    "                temp_ref = signed_int(sample[16:20])               \n",
    "                \n",
    "                rawpvalue = signed_int(sample[20:24])               \n",
    "                par = self.PARConversion(engr_meas=rawpvalue,\n",
    "                                        coef_offset=self.config['par_cal']['offset'], \n",
    "                                        coef_slope=self.config['par_cal']['slope']) \n",
    "                \n",
    "                rawfvalue = signed_int(sample[24:28])                 \n",
    "                fluor = self.FluorConversion(engr_meas=rawfvalue,\n",
    "                                        coef_offset=self.config['fluor_cal']['offset'], \n",
    "                                        coef_slope=self.config['fluor_cal']['slope'])  \n",
    "\n",
    "                tilt = int(sample[28:30],16) #degrees\n",
    "                \n",
    "            elif ((len(sample) == 34) or (len(sample) == 36) or (len(sample) == 72)) : #4byte timeword\n",
    "                \n",
    "                #seconds since 1970-01-01\n",
    "                time = int(sample[0:8],16)\n",
    "                \n",
    "                pressure =  self.PressureConversion(int(sample[8:12],16))   \n",
    "                \n",
    "                fr_temp_ADC = signed_int(sample[12:16])\n",
    "                fr_temp = self.TempConversion(engr_meas=fr_temp_ADC,\n",
    "                                                  coefA=self.config['fr_temp_cal']['Acoef'], \n",
    "                                                  coefB=self.config['fr_temp_cal']['Bcoef'],\n",
    "                                                  coefC=self.config['fr_temp_cal']['Ccoef'],\n",
    "                                                  version=2)\n",
    "\n",
    "                sr_temp_ADC = signed_int(sample[16:20])\n",
    "                sr_temp = self.TempConversion(engr_meas=sr_temp_ADC,\n",
    "                                                  coefA=self.config['sr_temp_cal']['Acoef'], \n",
    "                                                  coefB=self.config['sr_temp_cal']['Bcoef'],\n",
    "                                                  coefC=self.config['sr_temp_cal']['Ccoef'],\n",
    "                                                  version=2)\n",
    "\n",
    "                temp_ref_ADC = signed_int(sample[20:24])          \n",
    "                \n",
    "                rawpvalue = signed_int(sample[24:28])                 \n",
    "                par = self.PARConversion(engr_meas=rawpvalue,\n",
    "                                        coef_offset=self.config['par_cal']['offset'], \n",
    "                                        coef_slope=self.config['par_cal']['slope'])  \n",
    "\n",
    "                rawfvalue = signed_int(sample[28:32])               \n",
    "                fluor = self.FluorConversion(engr_meas=rawfvalue,\n",
    "                                        coef_offset=self.config['fluor_cal']['offset'], \n",
    "                                        coef_slope=self.config['fluor_cal']['slope'])  \n",
    "                \n",
    "                tilt = int(sample[32:34],16) #degrees            \n",
    "\n",
    "            elif len(sample) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                continue # skip lines with too many data points\n",
    "            #save to dictionary\n",
    "            data[sample_num] = {'time':time,\n",
    "                                'pressure':pressure,\n",
    "                                'fr_temp':fr_temp,\n",
    "                                'fr_temp_ADC':fr_temp_ADC,\n",
    "                                'sr_temp':sr_temp,\n",
    "                                'sr_temp_ADC':sr_temp_ADC,\n",
    "                                'temp_ref':temp_ref,\n",
    "                                'par':par,\n",
    "                                'fluor':fluor,\n",
    "                                'tilt':tilt}  \n",
    "        return(data)\n",
    "\n",
    "    @staticmethod\n",
    "    def PARConversion(engr_meas,coef_offset, coef_slope):\n",
    "        ''' Calculate PAR from raw measurement\n",
    "        \n",
    "            (ADC_val - coef_offset) * coef_slope / 0.73\n",
    "\n",
    "            output is PAR in umolm-2s-1\n",
    "        '''\n",
    "        return((engr_meas - coef_offset) * coef_slope / 0.73 )\n",
    "\n",
    "    @staticmethod\n",
    "    def FluorConversion(engr_meas,coef_offset, coef_slope):\n",
    "        ''' Calculate Fluometer from raw measurement\n",
    "\n",
    "            (ADC_val - coef_offset) * coef_slope\n",
    "        \n",
    "            output is concentration in ug/L\n",
    "        '''\n",
    "        return((engr_meas - coef_offset) * coef_slope) \n",
    "\n",
    "    @staticmethod\n",
    "    def PressureConversion(engr_meas):\n",
    "        ''' Calculate Pressure from raw measurement\n",
    "\n",
    "            (ADC_val - 16384) * 10 / 32768\n",
    "        \n",
    "            output is Pressure in Bars\n",
    "        '''\n",
    "        return((engr_meas - 16384) * 10 / 32768)\n",
    "\n",
    "    @staticmethod\n",
    "    def TempConversion(engr_meas, coefA, coefB, coefC, version=2):\n",
    "        ''' Calculate Temperature from raw measurement.\n",
    "\n",
    "        1 / ( coefA + \n",
    "              coefB*np.log(ADC_val) + \n",
    "              coefC*np.log(ADC_val)^3 ) - 273.15 \n",
    "\n",
    "        Output is Temperature in DegC\n",
    "        '''\n",
    "        if version == 1:\n",
    "            temperature = 1 / ( coefA + \n",
    "                         coefB*np.log10(engr_meas) + \n",
    "                         coefC*np.log10(engr_meas)**3 ) - 273.15 \n",
    "        if version == 2:\n",
    "            temperature = 1 / ( coefA + \n",
    "                         coefB*np.log(engr_meas) + \n",
    "                         coefC*np.log(engr_meas)**3 ) - 273.15   \n",
    "\n",
    "        return(temperature)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Evaluation of routine\n",
    "\n",
    "Imagine a buoy with ID number **xxxxx**.  Instantiate a PopUpBuoys class with the relative (or absolute) path to the location of the download/reconstructed data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDNUMBER = PopUpBuoys(root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the configuration file.  If not specified it will load a file named 'default.yaml' in the same path as the utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDNUMBER.LoadCoefs(config_file=i_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the routine to read and convert the binary file to a hex string... the sample parameter is the name of the data type.\n",
    "\n",
    "active_stream options are:\n",
    "+ bottom\n",
    "+ SST\n",
    "+ profile\n",
    "+ ice\n",
    "\n",
    "passing 'verbose=True' returns the hex string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOTTOM DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    IDNUMBER.HexView(active_stream='bottom',verbose=False)\n",
    "    IDNUMBER.HexSplit(verbose=False)\n",
    "    bottom_data = IDNUMBER.Bottom(asPandas=True)\n",
    "    print(bottom_data)\n",
    "except:\n",
    "    print ('No Bottom Data') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROFILE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    IDNUMBER.HexView(active_stream='profile',verbose=True)\n",
    "    IDNUMBER.HexSplit(verbose=True)\n",
    "    pro_data = IDNUMBER.Profile(asPandas=True)\n",
    "    print(pro_data)\n",
    "except:\n",
    "    print('No profile data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDNUMBER.Ice(asPandas=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNDER ICE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " try:  \n",
    "    IDNUMBER.HexView(active_stream='ice',verbose=False)\n",
    "    IDNUMBER.HexSplit(verbose=False)\n",
    "    ice_data = IDNUMBER.Ice(asPandas=True)\n",
    "    print(ice_data)\n",
    "except:\n",
    "    print('No under ice data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEA SURFACE DRIFTER DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    IDNUMBER.HexView(active_stream='sst',verbose=True)\n",
    "    IDNUMBER.HexSplit(verbose=True)\n",
    "    sst_data = IDNUMBER.SST(asPandas=True)\n",
    "    print(sst_data)\n",
    "except:\n",
    "    print('No sea surface drifter data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert elapse time to datetime\n",
    "### Profile data spells out the profile start time which is used to create the \"datetime\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time functions are in netcdf4 library\n",
    "try:\n",
    "    pro_data['datetime'] =[num2date(x['time'],'seconds since 1970-1-1') for i,x in pro_data.iterrows()]\n",
    "    print(pro_data['datetime'])\n",
    "except:\n",
    "    print('no profile data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottom and Ice data start marking time from the \"Unit_Start_Time\" and \"Unit_Release_Time\" which are in the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yaml.dump(IDNUMBER.config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    bottom_data['datetime'] =[num2date(x['time'],'seconds since '+ (IDNUMBER.config['Unit_Start_Time']),only_use_cftime_datetimes=False ) for i, x in bottom_data.iterrows()]\n",
    "    print(bottom_data['datetime'])\n",
    "except:\n",
    "    print('No bottom data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim out deck data using deploy date in config file for output csv files...currently throws an error...? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bottom_data = bottom_data.drop(bottom_data[bottom_data['datetime'] < IDNUMBER.config['Unit_Deploy_Time']].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ice_data['datetime'] =[num2date(x['time'],'seconds since '+ str(IDNUMBER.config['Unit_Release_Time']),only_use_cftime_datetimes=False ) for i,x in ice_data.iterrows()]\n",
    "    print(ice_data['datetime'])\n",
    "except:\n",
    "    print('No under ice data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sst_data['datetime'] =[num2date(x['time'],'seconds since 1970-1-1',only_use_cftime_datetimes=True ) for i,x in sst_data.iterrows()]\n",
    "    print(sst_data['datetime'])\n",
    "except:\n",
    "    print(\"No sea surface drifter data\")   \n",
    "#filter out any dates that are before the deployment date\n",
    "#sst_data = sst_data[sst_data.datetime>IDNUMBER.config['Unit_Start_Time']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Data to be used in other software\n",
    "\n",
    "simple as sending the pandas dataframe to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add columnes depth_m and id, lat, lon from config to datafiles\n",
    "try:\n",
    "    ice_data['depth_m'] = ice_data.pressure*9.931170631574\n",
    "    ice_data['id'] = IDNUMBER.config['IMEI_SN']\n",
    "    ice_data['dep_lat'] = IDNUMBER.config['Deploy_LatN']\n",
    "    ice_data['dep_lon'] = IDNUMBER.config['Deploy_LonW']\n",
    "    ice_data.to_csv(os.path.join(root_path, sn +'_ice_data.csv'))\n",
    "except:\n",
    "    print(\"No under ice data\")\n",
    "try:\n",
    "    sst_data['depth_m'] = sst_data.pressure*9.931170631574\n",
    "    sst_data['id'] = IDNUMBER.config['IMEI_SN']\n",
    "    sst_data['dep_lat'] = IDNUMBER.config['Deploy_LatN']\n",
    "    sst_data['dep_lon'] = IDNUMBER.config['Deploy_LonW']\n",
    "    sst_data.to_csv(os.path.join(root_path, sn +'_sst_data.csv'))\n",
    "except:\n",
    "    print(\"No sea surface drifter data\")\n",
    "try:\n",
    "    bottom_data['depth_m'] = bottom_data.pressure*9.931170631574\n",
    "    bottom_data['id'] = IDNUMBER.config['IMEI_SN']\n",
    "    bottom_data['dep_lat'] = IDNUMBER.config['Deploy_LatN']\n",
    "    bottom_data['dep_lon'] = IDNUMBER.config['Deploy_LonW']\n",
    "    bottom_data.to_csv(os.path.join(root_path, sn +'_bottom_data.csv'))\n",
    "except:\n",
    "    print(\"No bottom data\")  \n",
    "try:\n",
    "    pro_data['depth_m'] = pro_data.pressure*9.931170631574\n",
    "    pro_data['id'] = IDNUMBER.config['IMEI_SN']\n",
    "    pro_data['dep_lat'] = IDNUMBER.config['Deploy_LatN']\n",
    "    pro_data['dep_lon'] = IDNUMBER.config['Deploy_LonW']\n",
    "    pro_data.to_csv(os.path.join(root_path, sn +'_profile_data.csv'))\n",
    "except:\n",
    "    print(\"No profile data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOTTING FUN\n",
    "#### Since a pandas dataframe is a native output, quicklook plots are very easy to obtain as are output to csv for other software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "try:\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    ax1 = plt.subplot(1, 1, 1)\n",
    "    pd.plotting.scatter_matrix(pro_data, ax=ax1)\n",
    "    fig.savefig(os.path.join(root_path, sn+'_profile_data.facetgrid.png'), dpi=300, tight_layout=True)\n",
    "\n",
    "except:\n",
    "    print('no profile data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fig = plt.figure(1,figsize=(9,5))\n",
    "    ### CTD Temperature\n",
    "    ax1 = plt.subplot(1, 3, 1)\n",
    "    plt.plot(pro_data.fr_temp,pro_data.pressure*10,'r',linewidth=.5)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.set_xlabel('Temperature')\n",
    "    ax1.set_ylabel('Depth (m)')\n",
    "    ### CTD PAR\n",
    "    ax1 = plt.subplot(1, 3, 2)\n",
    "    plt.plot(pro_data.par,pro_data.pressure*10,'y',linewidth=.5)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.set_xlabel('PAR')\n",
    "    ### CTD Temperature\n",
    "    ax1 = plt.subplot(1, 3, 3)\n",
    "    plt.plot(pro_data.fluor,pro_data.pressure*10,'g',linewidth=.5)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.set_xlabel('Chlor')\n",
    "\n",
    "    fig.savefig( '..//results/'+ year + '/' + instid + '/'+ year + '/' + instid +'_pro_data.simple.png',dpi=300)\n",
    "\n",
    "except:\n",
    "    print('No profile data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeseries Plots of Bottom Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1,figsize=(8,2.5))\n",
    "### CTD Temperature\n",
    "ax1a = plt.subplot(3, 1, 1)\n",
    "plt.plot(bottom_data.datetime,bottom_data.fr_temp,'r',linewidth=.75)\n",
    "plt.plot(bottom_data.datetime,bottom_data.sr_temp,'m',linewidth=.5)\n",
    "ax1a.set_xticklabels([])\n",
    "ax1a.set_ylabel('Temp')\n",
    "### Par\n",
    "ax1b = plt.subplot(3, 1, 2)\n",
    "plt.plot(bottom_data.datetime,bottom_data.par,'y',linewidth=.75)\n",
    "ax1b.set_xticklabels([])\n",
    "ax1b.set_ylabel('PAR')\n",
    "### Chlor\n",
    "ax1c = plt.subplot(3, 1, 3)\n",
    "plt.plot(bottom_data.datetime,bottom_data.fluor,'g',linewidth=.75)\n",
    "ax1c.set_ylabel('Chlor')\n",
    "\n",
    "\n",
    "## specify datelabels and ticks for all three pannels\n",
    "ax1a.yaxis.set_minor_locator(ticker.MultipleLocator(1))\n",
    "ax1a.xaxis.set_major_locator(DayLocator(bymonthday=1))\n",
    "ax1a.xaxis.set_minor_locator(DayLocator(bymonthday=15))\n",
    "#ax1b.yaxis.set_minor_locator(ticker.MultipleLocator(2.5))\n",
    "ax1b.xaxis.set_major_locator(DayLocator(bymonthday=1))\n",
    "ax1b.xaxis.set_minor_locator(DayLocator(bymonthday=15))\n",
    "\n",
    "ax1c.tick_params(labelbottom=True,labeltop=False)\n",
    "#ax1c.yaxis.set_minor_locator(ticker.MultipleLocator(2.5))\n",
    "ax1c.xaxis.set_major_locator(DayLocator(bymonthday=1))\n",
    "ax1c.xaxis.set_minor_locator(DayLocator(bymonthday=15))\n",
    "ax1c.xaxis.set_major_formatter(ticker.NullFormatter())\n",
    "ax1c.xaxis.set_minor_formatter(DateFormatter('%b %Y'))\n",
    "ax1c.xaxis.set_major_formatter(DateFormatter(''))\n",
    "ax1c.xaxis.set_tick_params(which='major', pad=15)\n",
    "ax1c.xaxis.set_tick_params(which='minor', pad=5)\n",
    "\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "fig.savefig(os.path.join(root_path, sn+'_bottom_data.png'),dpi=300)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bottom Temp Detail*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bottom_data.datetime,bottom_data.fr_temp,'r',linewidth=.75, label='fr fast response temp')\n",
    "plt.plot(bottom_data.datetime,bottom_data.sr_temp,'m',linewidth=.5, label='sr slow response temp')\n",
    "plt.ylabel('Temp C')\n",
    "plt.xlabel('Date')\n",
    "plt.title(sn + '_Bottom_Temperature')\n",
    "plt.xticks(rotation=30)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.savefig(os.path.join(root_path, sn+'_temp_detail_bottom_data.png'),dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeseries Plots of Under Ice Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1,figsize=(8,2.5))\n",
    "### CTD Temperature\n",
    "ax1a = plt.subplot(3, 1, 1)\n",
    "plt.plot(ice_data.datetime,ice_data.fr_temp,'r',linewidth=.75)\n",
    "plt.plot(ice_data.datetime,ice_data.sr_temp,'m',linewidth=.5)\n",
    "ax1a.set_xticklabels([])\n",
    "ax1a.set_ylabel('Temp')\n",
    "### Par\n",
    "ax1b = plt.subplot(3, 1, 2)\n",
    "plt.plot(ice_data.datetime,ice_data.par,'y',linewidth=.75)\n",
    "ax1b.set_xticklabels([])\n",
    "ax1b.set_ylabel('PAR')\n",
    "### Chlor\n",
    "ax1c = plt.subplot(3, 1, 3)\n",
    "plt.plot(ice_data.datetime,ice_data.fluor,'g',linewidth=.75)\n",
    "ax1c.set_ylabel('Chlor')\n",
    "\n",
    "## specify datelabels and ticks for all three pannels\n",
    "ax1a.yaxis.set_minor_locator(ticker.MultipleLocator(2.5))\n",
    "ax1a.xaxis.set_major_locator(DayLocator(bymonthday=1))\n",
    "ax1a.xaxis.set_minor_locator(DayLocator(bymonthday=15))\n",
    "ax1b.yaxis.set_minor_locator(ticker.MultipleLocator(2.5))\n",
    "ax1b.xaxis.set_major_locator(DayLocator(bymonthday=1))\n",
    "ax1b.xaxis.set_minor_locator(DayLocator(bymonthday=15))\n",
    "\n",
    "ax1c.tick_params(labelbottom=True,labeltop=False)\n",
    "ax1c.yaxis.set_minor_locator(ticker.MultipleLocator(2.5))\n",
    "ax1c.xaxis.set_major_locator(DayLocator(bymonthday=1))\n",
    "ax1c.xaxis.set_minor_locator(DayLocator(bymonthday=15))\n",
    "ax1c.xaxis.set_major_formatter(ticker.NullFormatter())\n",
    "ax1c.xaxis.set_minor_formatter(DateFormatter('%b %y'))\n",
    "ax1c.xaxis.set_major_formatter(DateFormatter(''))\n",
    "ax1c.xaxis.set_tick_params(which='major', pad=15)\n",
    "ax1c.xaxis.set_tick_params(which='minor', pad=5)\n",
    "\n",
    "fig.subplots_adjust(bottom=0.1)\n",
    "fig.savefig(os.path.join(root_path, sn+'_ice_data.png'),dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ice temp detail*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ice_data.datetime,ice_data.fr_temp,'r',linewidth=.75, label='fr fast response temp')\n",
    "plt.plot(ice_data.datetime,ice_data.sr_temp,'b',linewidth=.5, label='sr slow response temp')\n",
    "plt.ylabel('Temp C')\n",
    "plt.xlabel('Date')\n",
    "plt.title(sn + '_Under_Ice_Temperature')\n",
    "plt.xticks(rotation=30)\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.savefig(os.path.join(root_path, sn+'_temp_detail_ice_data.png'),dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeseries of Sea Surface Drifter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fig = plt.figure(1,figsize=(8,2.5))\n",
    "    ### CTD Temperature\n",
    "    ax1a = plt.subplot(3, 1, 1)\n",
    "    plt.plot(SST_data.datetime,SST_data.fr_temp,'r.',markersize=.5,linewidth=.75)\n",
    "    plt.plot(SST_data.datetime,SST_data.sr_temp,'m.',markersize=.5,linewidth=.5)\n",
    "    ax1a.set_xticklabels([])\n",
    "    ax1a.set_ylabel('Temp')\n",
    "    ### Par\n",
    "    ax1b = plt.subplot(3, 1, 2)\n",
    "    plt.plot(SST_data.datetime,SST_data.par,'y.',markersize=.5,linewidth=.75)\n",
    "    ax1b.set_xticklabels([])\n",
    "    ax1b.set_ylabel('PAR')\n",
    "    ### Chlor\n",
    "    ax1c = plt.subplot(3, 1, 3)\n",
    "    plt.plot(SST_data.datetime,SST_data.fluor,'g.',markersize=.5,linewidth=.75)\n",
    "    ax1c.set_ylabel('Chlor')\n",
    "\n",
    "\n",
    "    ## specify datelabels and ticks for all three pannels\n",
    "    ax1a.yaxis.set_minor_locator(ticker.MultipleLocator(2.5))\n",
    "    ax1a.xaxis.set_major_locator(DayLocator(bymonthday=1))\n",
    "    ax1a.xaxis.set_minor_locator(DayLocator(bymonthday=15))\n",
    "    ax1b.yaxis.set_minor_locator(ticker.MultipleLocator(2.5))\n",
    "    ax1b.xaxis.set_major_locator(DayLocator(bymonthday=1))\n",
    "    ax1b.xaxis.set_minor_locator(DayLocator(bymonthday=15))\n",
    "\n",
    "    ax1c.tick_params(labelbottom=True,labeltop=False)\n",
    "    ax1c.yaxis.set_minor_locator(ticker.MultipleLocator(2.5))\n",
    "    ax1c.xaxis.set_major_locator(DayLocator(bymonthday=1))\n",
    "    ax1c.xaxis.set_minor_locator(DayLocator(bymonthday=15))\n",
    "    ax1c.xaxis.set_major_formatter(ticker.NullFormatter())\n",
    "    ax1c.xaxis.set_minor_formatter(DateFormatter('%b %y'))\n",
    "    ax1c.xaxis.set_major_formatter(DateFormatter(''))\n",
    "    ax1c.xaxis.set_tick_params(which='major', pad=15)\n",
    "    ax1c.xaxis.set_tick_params(which='minor', pad=5)\n",
    "\n",
    "    fig.savefig(os.path.join(root_path, sn+'_sea-surface-drifter_data.png'),dpi=300, tight_layout=True)\n",
    "except:\n",
    "    print(\"No sea surface drifter data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
